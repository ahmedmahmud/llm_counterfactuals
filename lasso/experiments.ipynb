{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a06d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f72a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"google/gemma-2-2b-it\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LanguageModel(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90cdca95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612a9e79f8f244e5aaa3f23935fe4dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "This is a beatiful car.\n",
      "Is the above statement positive in sentiment? Answer Yes or No only<end_of_turn>\n",
      "<start_of_turn>modelYes \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"\"\"<start_of_turn>user\n",
    "This is a beatiful car.\n",
    "Is the above statement positive in sentiment? Answer Yes or No only<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "max_new_tokens = 50\n",
    "\n",
    "with model.generate(prompt_text, max_new_tokens=max_new_tokens) as gen_tracer:\n",
    "    output_tokens = model.generator.output.save()\n",
    "\n",
    "generated = model.tokenizer.decode(output_tokens[0].cpu())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b52958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 top-10 predictions:\n",
      "  No ('No'): 1.0000\n",
      "  Yes ('Yes'): 0.0000\n",
      "  Negative ('Negative'): 0.0000\n",
      "  no ('no'): 0.0000\n",
      "  NO ('NO'): 0.0000\n",
      "  False ('False'): 0.0000\n",
      "  Answer ('Answer'): 0.0000\n",
      "  Nope ('Nope'): 0.0000\n",
      "  ‚ñÅNo (' No'): 0.0000\n",
      "  N ('N'): 0.0000\n",
      "Step 2 top-10 predictions:\n",
      "  ‚ñÅ (' '): 0.9822\n",
      "  . ('.'): 0.0178\n",
      "  \n",
      " ('\\n'): 0.0001\n",
      "  <end_of_turn> ('<end_of_turn>'): 0.0000\n",
      "  ‚ñÅ‚ñÅ ('  '): 0.0000\n",
      "  <eos> ('<eos>'): 0.0000\n",
      "  , (','): 0.0000\n",
      "  ‚ñÅüòä (' üòä'): 0.0000\n",
      "  \n",
      "\n",
      " ('\\n\\n'): 0.0000\n",
      "  ‚ñÅüëç (' üëç'): 0.0000\n",
      "Step 3 top-10 predictions:\n",
      "  \n",
      " ('\\n'): 1.0000\n",
      "  \n",
      "\n",
      " ('\\n\\n'): 0.0000\n",
      "  \n",
      "\n",
      "\n",
      " ('\\n\\n\\n'): 0.0000\n",
      "  <end_of_turn> ('<end_of_turn>'): 0.0000\n",
      "  üöó ('üöó'): 0.0000\n",
      "  ‚ùå ('‚ùå'): 0.0000\n",
      "  üòú ('üòú'): 0.0000\n",
      "  <eos> ('<eos>'): 0.0000\n",
      "  üö´ ('üö´'): 0.0000\n",
      "  üòà ('üòà'): 0.0000\n",
      "Step 4 top-10 predictions:\n",
      "  <end_of_turn> ('<end_of_turn>'): 1.0000\n",
      "  ‚ñÅ (' '): 0.0000\n",
      "  ‚ñÅ‚ñÅ ('  '): 0.0000\n",
      "  ** ('**'): 0.0000\n",
      "  Let ('Let'): 0.0000\n",
      "  <eos> ('<eos>'): 0.0000\n",
      "  Provide ('Provide'): 0.0000\n",
      "  Please ('Please'): 0.0000\n",
      "  ‚ñÅPhry (' Phry'): 0.0000\n",
      "  Deny ('Deny'): 0.0000\n"
     ]
    },
    {
     "ename": "NNsightError",
     "evalue": "Accessing value before it's been set.",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"/home/ahmed/work/llm-counterfactuals/.venv/lib/python3.12/site-packages/nnsight/tracing/graph/node.py\", line 294, in execute",
      "    args, kwargs = self.prepare_inputs((self.args, self.kwargs))",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/home/ahmed/work/llm-counterfactuals/.venv/lib/python3.12/site-packages/nnsight/intervention/graph/node.py\", line 72, in prepare_inputs",
      "    inputs = util.apply(",
      "             ^^^^^^^^^^^",
      "  File \"/home/ahmed/work/llm-counterfactuals/.venv/lib/python3.12/site-packages/nnsight/util.py\", line 44, in apply",
      "    return tuple([apply(_data, fn, cls, inplace=inplace) for _data in data])",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/home/ahmed/work/llm-counterfactuals/.venv/lib/python3.12/site-packages/nnsight/util.py\", line 39, in apply",
      "    data[idx] = apply(_data, fn, cls, inplace=inplace)",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/home/ahmed/work/llm-counterfactuals/.venv/lib/python3.12/site-packages/nnsight/util.py\", line 32, in apply",
      "    return fn(data)",
      "           ^^^^^^^^",
      "  File \"/home/ahmed/work/llm-counterfactuals/.venv/lib/python3.12/site-packages/nnsight/intervention/graph/node.py\", line 65, in inner",
      "    value = value.value",
      "            ^^^^^^^^^^^",
      "  File \"/home/ahmed/work/llm-counterfactuals/.venv/lib/python3.12/site-packages/nnsight/tracing/graph/node.py\", line 143, in value",
      "    raise ValueError(\"Accessing value before it's been set.\")",
      "ValueError: Accessing value before it's been set.",
      "",
      "NNsightError: Accessing value before it's been set."
     ]
    }
   ],
   "source": [
    "prompt_text = \"\"\"<start_of_turn>user\n",
    "This is a ugly car.\n",
    "Is the above statement positive in sentiment? Answer Yes or No only<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "n_steps = 5   # how many tokens to analyze\n",
    "k = 10        # top-k predictions per step\n",
    "\n",
    "# Run generation context\n",
    "with model.generate(prompt_text, max_new_tokens=n_steps) as _:\n",
    "    # List to hold proxies for each step's logits\n",
    "    logits_proxies = []\n",
    "    for _ in range(n_steps):\n",
    "        # Save current logits (for next-token prediction)\n",
    "        logits_proxies.append(model.lm_head.output.save())\n",
    "        # Advance the generator by one token\n",
    "        model.lm_head.next()\n",
    "\n",
    "# After exiting the context, extract real tensors and compute top-k\n",
    "for step_idx, proxy in enumerate(logits_proxies, start=1):\n",
    "    logits = proxy      # [1, seq_len + step_idx - 1, vocab_size]\n",
    "    last_logits = logits[0, -1]\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    topk_probs, topk_ids = torch.topk(probs, k)\n",
    "    raw_tokens = model.tokenizer.convert_ids_to_tokens(topk_ids.tolist())\n",
    "    decoded = [model.tokenizer.decode([tid]) for tid in topk_ids.tolist()]\n",
    "\n",
    "    print(f\"Step {step_idx} top-{k} predictions:\")\n",
    "    for tok, rep, p in zip(raw_tokens, decoded, topk_probs.tolist()):\n",
    "        print(f\"  {tok} ({repr(rep)}): {p:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
